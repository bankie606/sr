\documentclass[12pt]{article}

\usepackage{fullpage}

\title{Sr: Processing Data Streams By Doing Less Work}
\author{Ryan Lopopolo\\
\texttt{lopopolo@mit.edu}}

\begin{document}
\twocolumn
\maketitle


\section{Design}
Sr is a distributed system for processing streams that can choose to kill tasks or limit the amount of work it performs per task. Its design is modular; there are five different types of nodes in an Sr cluster (spout, fetcher, worker, collector, master).
\subsection{Spout}
Spouts are network-connected machines that generate data for the Sr cluster to process. Examples include a machine that tails a log file or an API like the Twitter tweet stream. Fetchers depend on spouts as a source of streaming data.
\subsection{Fetcher}
Fetchers are responsible for communicating with spouts and returning data for the workers to process. Fetchers may run indefinitely or return a status code indicating the fetching task is over.
\subsection{Worker}
Workers perform processing on data provided by fetchers. They are similar to mappers in the MapReduce framework. Each worker receives a fraction of the total input data and each worker’s tasks are independent of other workers’.

Workers are responsible for fulfilling the main design goals of Sr. Workers may choose to kill a task preemptively or if it does not complete before a timeout. Additionally, workers may choose among a variety of routines for performing computation on a datum. Routines are ranked on a [0,1] scale based on the estimated fraction of total work it requires.

As it runs, a worker maintains an internal metric that estimates the fraction of work it has performed over all data it has seen. Workers use this metric when choosing which computation routine to run for a datum. A worker will choose the lowest ranked routine such that its estimated fraction of work is above a configured threshold.

Because one of Sr’s goals is to process (potentially infinite) streams, workers do not save their progress to disk and are forgetful; once the master queries a worker for its progress and the worker successfully returns it, the worker clears all of its state and continues processing. As a consequence, workers are easy to add or remove from the worker pool because they don’t need to be initialized with long-term job state. Additionally, as long as the master polls workers frequently enough, when a worker fails, only a small fraction of the total work is lost. The master queries workers every 100ms by default.
\subsection{Collector}
Collector nodes aggregate the results of all workers. Each collector acts as a reducer in the MapReduce framework if MapReduce used a single reducer for all of the mappers. Collectors differ from MapReduce reducers in that they receive their data piecewise, as it is produced from workers as opposed to after all mappers have completed. This limits the type of aggregation collectors can do to online operations.

Collectors also expose an API endpoint for accessing the results of a job. This endpoint can be used by another spout to chain Sr jobs together.
\subsection{Master}
There is one master node per Sr network. It has a function similar to the jobtracker in MapReduce. New jobs are scheduled with the master and it initializes other nodes in the cluster with job state. For each job, the master schedules roles among nodes in a round robin manner to minimize the total number of outstanding roles on any one node.

The master is the only well-known node in the cluster. Other nodes in the network exclusively communicate with the master. The master and other nodes communicate with each other by issuing HTTP requests to a simple server on each node. I decided to make use of a library [Sinatra.rb] providing the server functionality because there was a low barrier to getting inter-node communication functioning. This design choice presents some problems and will be discussed further in section 5.

Another consequence of the master being the only well-known node is that all data in the cluster must flow through it. The master maintains two queues per job for buffering job progress from fetchers and workers and periodically pushes this state out to workers and collectors, respectively.
\subsection{Configuring a Cluster for a Job}
Apart from spouts, all nodes in a cluster modify their behavior based on the job they are running. The master must know the appropriate number of nodes to spin up; fetchers need to know how to talk to spouts; workers need to know how to compute results; and collectors need to know how to combine batched worker results.

Sr accomplishes this configuration with a Jobfile. Jobfiles are instances of a special class provided by the Sr framework. This class exposes several methods that can be invoked by a given node to provide configuration parameters or execute code on its behalf. Most of the routines that advance the progress of an Sr job are wrappers around these Jobfile methods.

Sr sends the source of the Jobfile to every node participating in the job and uses eval to obtain a local instance of the Jobfile class. An example jobfile can be found at [github].

The use of eval to instantiate Jobfile objects and the implementation language’s (Ruby) ability to monkeypatch classes means that jobs can be modified on the fly just by sending an updated Jobfile to job participants. For example, a job can be made lower fidelity in the face of an impending time constraint or higher fidelity if more workers are added to the cluster.
\subsection{Fault Tolerance}
Sr can sustain the failure of any node other than the master. Because the master is responsible for shuffling data between fetchers, workers, and collectors, a job cannot make progress without the master.

Fetchers can fail. The only consequence of fetcher failure is that some of the input stream is lost forever. Since streams contain much data and are likely stored permanently elsewhere on disk, Sr does not attempt to recover this data.

Worker failures may entail some lost work. Having the master poll each worker frequently for its progress mitigates the consequences of worker failure. For jobs that run for minutes or hours, 100ms of lost work is << 0.1% of the total. Sr can simply treat these lost tasks as preemptively killed, which is already expected by the cluster.

Each collector is an exact replica of all others. Collector failure does not impact correctness of the results delivered by any other collector.
\subsection{Killing Tasks and Working Less}
A design goal of Sr is to compute meaningful results from a stream and to do so while not processing every datum one-to-one. Sr achieves this goal in two different ways. First, workers may kill tasks preemptively to avoid the time and resource costs associated with processing that task. Second, workers rely on the Jobfile to provide them with several different computation routines of differing fidelities that perform a fraction of the total work on a datum.

These behaviors appeal to the law of large numbers. It assumes that trends will still be visible in spite of looking at only a fraction of the data. This means that the computation routines and the weights assigned to them must be adapted to fit the distribution of the data before the job starts (or updated in situ via a new Jobfile).

\section{Methodology}
The cluster I used to test Sr is a 5-node network consisting of 1 spout, 1 fetcher, 4 workers, 1 collector, and 1 master. Roles are partitioned among nodes as shown in figure 1. Worker nodes were XVM [SIPB] VMs provisioned with 128MB RAM each. The master, spout, fetcher, and collector ran on my laptop, a mid-2010 Macbook Air with 4GB of RAM.

\begin{figure}
\begin{verbatim}
                  |--------|
                - | worker | 
               /  |--------|
|-----------| /   |--------|
| master    | --- | worker |
| spout     |     |--------|
| fetcher   |     |--------|
| collector | --- | worker |
|-----------| \   |--------|
               \  |--------|
                - | worker |
                  |--------|

\end{verbatim}
\caption{Topology of the test Sr cluster. Each box is a single machine.}
\end{figure}

The job I ran was a word counter. The collector emits a hash whose keys are words and whose values are the frequency that word has occurred in the test corpus.

To run the job, I had to create a spout and find a source of data. For data, I acquired a Wikipedia dump of the Featured Articles category. I used Wikipedia’s xml exporter [Special:export] on January 9, 2012.

To serve the data, I munged it into a YAML file containing a hash of \texttt{article\_id => fulltext}. To store fulltext, I base64 encoded it and deflated it. This requires workers to inflate and decode the fulltext before they can process it further. The server reads from this YAML file and returns the resulting hash as JSON.

The fetcher communicates to the spout by requesting sequential article ids until it has fetched every article.

Workers are initialized with the 3 computation routines shown in table 1.

\begin{center}
\begin{table}
\begin{tabular}{|l|p{5cm}|}
 \hline
Weight & Description \\ \hline
1.0 & Deflate and decode fulltext and count words in all of fulltext \\ \hline
0.25 & Deflate and decode fulltext and count words in approximately a quarter of fulltext \\ \hline
0.1 & Return the results of the last computation from a cache \\ \hline
\end{tabular}
\caption{Computation routines used by Sr workers}
\end{table}
\end{center}


\end{document}