\documentclass[12pt,twocolumn]{article}

\usepackage{fullpage}
\usepackage{listings}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{tabularx}
\usepackage{url}

\title{StreamReduce: Processing Data Streams\\and Avoiding Work}
\author{Ryan Lopopolo\\
Massachusetts Institute of Technology\\
\texttt{lopopolo@mit.edu}}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}
One of the fundamental problems MapReduce was created to solve was processing
the growing amount of data in the world. Now with the open source Apache Hadoop
and Pig~\cite{Olston:2008:PLN:1376616.1376726}, it is easy to process and query
the terabytes of data an organization
may collect. With terabytes of data, however, it is scarcely necessary to examine
every entry to uncover trends, especially when one only cares about the most prominent
trends or results. This type of workflow lends itself naturally to top-k-style computations,
data sampling tasks, and other cases where perfect-fidelity results are not required.
%Google runs PageRank on the entire Internet, but if
%it only did on half of the pages it crawls, it would likely still have a usable
%index for delivering the first 5 pages of results.

How does one decide which data to process then? Selctively deciding which data to
log is problematic because it can systemically mask some of the trends one would
try to uncover through analysis. In this paper, I explore the possibility of
discarding data randomly while trying to maintain a target accuracy level.
On the subset of data that I do process, I explore using
computation routines of differing fidelities to sample the data. Combining both of these
techniques allows the cluster to avoid performing work.

One of the problems with MapReduce is how to
incorporate new data into an already analyzed task. MapReduce simply says to rerun
the job with the new data included. Others have solved this problem by batching data
into chunks and continuously spawning MapReduce jobs to incorporate the new data.

A simpler way to incorporate new data is to treat data not as a constant, but as a
stream. By processing data on demand, one avoids repeating work involved in reprocessing
data, can save resources
by spinning workers up and down as necessary, and ensure results are as fresh as possible
by incorporating data into the result as soon as it arrives.

The goal of StreamReduce is to provide a MapReduce-style framework for processing streams. Such a
framework should include the ability to kill or discard tasks and be capable of
providing workers with serveral different computation routines to choose among.

The remainder of this paper is organized as follows. Section~\ref{sec:relwork} provides
an overview of related work on distributed stream processing as well as dynamic and
approximate computations. Section~\ref{sec:design} details how I designed StreamReduce.
Section~\ref{sec:methodology} explains the methodology used to perform a cvase study
using StreamReduce. Section~\ref{sec:evaluation} evaluates the effectiveness of StreamReduce in the case study.
Section~\ref{sec:discussion} enumerates issues with StreamReduce's current implementation.

\section{Related Work}
\label{sec:relwork}
% data stream management system
% continuous queries
%   sr doesn't do this, instead it operates on snapshots
%   this is how online mapreduce works
% append only data sets == Chronicle data model
%   views maintained incrementally without storing any chronicles (data items)
% sequencing of streams and sliding windows
%   sr doesn't do this
% Aurora - load shedding based on app specific QoS
% approximate answers
%   baked in to StreamReduce
%

StreamReduce builds of previous work in the areas of MapReduce and Data Stream Management Systems.

\subsection{MapReduce}
There is a rich research field around MapReduce. This section details work related to adapting
MapReduce to a streaming environment.

The Hadoop Online Prototype~\cite{Condie:EECS-2009-136} adds streaming to the MapReduce framework
by removing the materialize phase between mappers and reducers and between phases of a
MapReduce pipeline. Results are streamed from mappers to reducers as they become available.
At any given point, each reducer maintains an in-progress snapshot of the MapReduce job's
progress up to that point. This removes the synchronization barriers inherient in MapReduce
and allows incremental progress to be observed.

This approach is in contrast to the continuous query approach taken by many other streaming
systems.

\subsection{Data Stream Management Systems}
In their overview on data stream projects~\cite{Babcock:2002:MID:543613.543615},
a new class of data streaming applications
called \emph{Data Stream Management Systems} (\emph{DSMSes}) are discussed. StreamReduce is a type of DSMS that
includes some properties found in other systems.

The \emph{Chronicle Data Model}~\cite{Jagadish:1995:VMI:212433.220201} describes an
append-only stream of tuples. Such
streams lend themselves to incremental reads and incremental processing. In addition to incremental
updates, the Chronicle Data Model also seeks to update results incrementally without storing the
data itself.

The \emph{Data Stream Model}~\cite{Babcock:2002:MID:543613.543615} is a refinement of the Chronicle
Data Model. It has the following properties: stream items may arrive out of order,
streams are potentially unbounded in size, elements in the stream are discared after they are
processed. This is the data model that StreamReduce uses.

The Aurora system~\cite{Carney:2002:MSN:1287369.1287389} introduces the concept of
\emph{load shedding}, or compensating for
resource overload by comparing job specific quality of service metrics with target levels.

\section{Design}
\label{sec:design}
StreamReduce is a distributed system for processing streams that can choose to kill tasks or
limit the amount of work it performs per task. Its design is modular and there are five
different types of nodes in an StreamReduce cluster: spout, fetcher, worker, collector, master.
\subsection{Spout}
Spouts are network-connected machines that generate data for an StreamReduce cluster to process.
Examples include a machine that tails a log file or an API like the Twitter tweet
stream. Fetchers depend on spouts as a source of streaming data.
\subsection{Fetcher}
Fetchers are responsible for communicating with spouts and returning data for the
workers to process. Fetchers may run indefinitely or return a status code indicating
the fetching task is over.
\subsection{Worker}
Workers perform processing on data provided by fetchers. They are similar to mappers
in the MapReduce framework. Each worker receives a fraction of the total input data
and each worker's tasks are independent of other workers'.

Workers are responsible for fulfilling the main design goals of StreamReduce. Workers may choose
to kill a task preemptively or greedily if it does not complete before a timeout. Additionally,
workers may choose among a variety of routines for performing computation on a datum.
Routines are ranked on a [0,1] scale based on the estimated fraction of total work it
requires.

As it runs, a worker maintains an internal metric that estimates the fraction of work
it has performed over all data it has seen. Workers use this metric when choosing which
computation routine to run for a datum. A worker will choose the lowest ranked routine
such that its estimated fraction of work is above a configured threshold.
The total work estimator is used as a proxy for accuracy when a worker updates
this metric. This metric also takes into account the number of tasks it has killed
for any reason.

Because one of StreamReduce's goals is to process (potentially infinite) streams, workers do not
save their progress to disk and are forgetful; once the master queries a worker for its
progress and the worker successfully returns it, the worker clears all of its state and
continues processing. As a consequence, workers are easy to add or remove from the worker
pool because they don't need to be initialized with long-term job state. Additionally, as
long as the master polls workers frequently enough, when a worker fails, only a small
fraction of the total work is lost. The master queries workers every 100ms by default.
\subsection{Collector}
Collector nodes aggregate the results of all workers. Each collector acts as a reducer would
in the MapReduce framework if MapReduce used a single reducer for all of the mappers.
Collectors differ from MapReduce reducers in that they receive their data piecewise, as it
is produced from workers as opposed to after all mappers have completed. This limits the
types of stateless aggregation collectors can do to online operations. (Of course, a collector
can choose to retain copies of individual worker results and re-reduce them each time more
results are receieved, but this is not in the spirit of the framework.)

Collectors also expose an API endpoint for accessing the results of a job. This endpoint
can be used by another spout to chain StreamReduce jobs together.
\subsection{Master}
\label{sec:master}
There is one master node per StreamReduce network. It has a function similar to the jobtracker in
MapReduce. New jobs are scheduled with the master and it initializes other nodes in the
cluster with job state. For each job, the master schedules roles among nodes in a round robin
manner to minimize the total number of outstanding roles on any one node.

The master is the only well-known node in the cluster. Other nodes in the network exclusively
communicate with the master. The master and other nodes communicate with each other by issuing
HTTP requests to a simple server on each node. I decided to make use of a library~\cite{sinatrarb}
providing the server functionality because there was a low barrier to getting inter-node
communication functioning. This design choice presents some problems and will be discussed
further in Section~\ref{sec:discussion}.

Another consequence of the master being the only well-known node is that all data in the
cluster must flow through it. The master maintains two queues per job for buffering job
progress from fetchers and workers and periodically pushes this state out to workers and
collectors, respectively.
\subsection{Configuring a Cluster for a Job}
\label{sec:jobfile}
Apart from spouts, all nodes in a cluster modify their behavior based on the job they are
running. The master must know the appropriate number of nodes to spin up; fetchers need to
know how to talk to spouts; workers need to know how to compute results; and collectors need
to know how to combine batched worker results.

StreamReduce achieves this configuration with a Jobfile. Jobfiles are instances of a special class
provided by the StreamReduce framework. This class exposes several methods that can be invoked by a
given node to provide configuration parameters or execute code on its behalf. Most of the
routines that advance the progress of an StreamReduce job are wrappers around these Jobfile methods.

StreamReduce sends the source of the Jobfile to every node participating in the job and uses eval to
obtain a local instance of the Jobfile class. An example jobfile can be found at~\cite{github}.

The use of eval to instantiate Jobfile objects and the implementation language's (Ruby)
ability to monkeypatch classes means that jobs can be modified on the fly just by sending
an updated Jobfile to job participants. For example, a job can be made lower fidelity in
the face of an impending time constraint, higher fidelity if more workers are added to
the cluster, or gracefully killing fetchers which allows the job to end after all of the
various job queues are empty.
\subsection{Fault Tolerance}
StreamReduce can sustain the failure of any node other than the master. Because the master is responsible
for shuffling data between fetchers, workers, and collectors, a job cannot make progress without
the master.

Fetchers can fail. The only consequence of fetcher failure is that some of the input stream is
lost forever. Since streams contain much data and are possibly stored permanently elsewhere on disk
StreamReduce does not attempt to recover this data.

Worker failures may entail some lost work. Having the master poll each worker frequently for its
progress mitigates the consequences of worker failure. For jobs that run for minutes or hours,
100ms of lost work is $\ll0.1\%$ of the total. StreamReduce can simply treat these lost tasks as preemptively
killed, which is already expected by the cluster.

Each collector is an exact replica of all others. Collector failure does not impact correctness of
the results delivered by any other collector. If no collectors are online, the data meant for them
sits in a queue on the master. Bringing another collector online will allow these and future results
to be aggregated.
\subsection{Killing Tasks and Working Less}
A design goal of StreamReduce is to compute meaningful results from a stream and to do so while not
processing every datum one-to-one. StreamReduce achieves this goal in two different ways. First, workers
may kill tasks preemptively to avoid the time and resource costs associated with processing
that task. Second, workers rely on the Jobfile to provide them with several different
computation routines of differing fidelities that perform a fraction of the total work on a
datum.

These behaviors appeal to the law of large numbers. It assumes that trends will still be
visible in spite of looking at only a fraction of the data. This means that the computation
routines and the weights assigned to them must be adapted to fit the distribution of the data
before the job starts (or updated in situ via a new Jobfile).

\section{Methodology}
\label{sec:methodology}
The cluster I used to test StreamReduce is a 5-node network consisting of~1 spout,~1 fetcher,~4 workers,~1
collector, and~1 master. Roles were partitioned among nodes as shown in
Figure~\ref{fig:clusterDiagram}. Worker nodes were XVM~\cite{sipb:xvm} VMs provisioned with~128MB RAM each.
The master, spout, fetcher, and collector ran on a mid-2010 Macbook Air laptop with~4GB of RAM.

\begin{figure}
\begin{verbatim}
                  |--------|
                - | worker |
               /  |--------|
|-----------| /   |--------|
| master    | --- | worker |
| spout     |     |--------|
| fetcher   |     |--------|
| collector | --- | worker |
|-----------| \   |--------|
               \  |--------|
                - | worker |
                  |--------|
\end{verbatim}
\caption{Topology of the test StreamReduce cluster used in the word count case study. Each box represents a single machine.}
\label{fig:clusterDiagram}
\end{figure}

The job I ran was a word counter. The collector emits a hash whose keys are words and whose
values are the frequency the key occurred in the test corpus.

To run the job, I had to create a spout and find a source of data. For data, I acquired a
Wikipedia dump of the Featured Articles category. I used Wikipedia's xml
exporter~\cite{wikipedia:special:export} on January~9,~2012.

To serve the data, I munged it into a YAML file containing a hash of
\begin{center}
$\texttt{article\_id}\rightarrow\texttt{fulltext}$.
\end{center}
To store fulltext, I base64 encoded it and
deflated it using zlib. This requires workers to inflate and decode the fulltext before they
can process it further. The server reads from this YAML file and returns the resulting hash as
JSON.

The fetcher communicates to the spout by requesting sequential article ids until it has fetched
every article.

Workers are initialized with the~3 computation routines shown in Table~\ref{table:workerTasks}.

\begin{table}
\begin{tabularx}{\linewidth}{|c|X|}
 \hline
Weight & Description \\ \hline
1.0 & Deflate and decode fulltext and count words in all of fulltext \\ \hline
0.25 & Deflate and decode fulltext and count words in approximately a quarter of fulltext \\ \hline
0.1 & Return the results of the last computation from a cache \\ \hline
\end{tabularx}
\caption{Computation routines used by StreamReduce workers.}
\label{table:workerTasks}
\end{table}

The results of the word count job were then used in a script to compute the top~100 words.
The cluster and word count job could be used in a two-stage top-k job. This script filters
out SEO stopwords~\cite{stopwords} to achieve more interesting results.

\section{Evaluation}
\label{sec:evaluation}
To evaluate StreamReduce, I explore the precision and recall of the word count test job as well as the
speedup factor associated with varying the kill rate and target work level.

\subsection{Precision and Recall}
In order for task killing to be an effective strategy for a top-k computation, the
distribution of words in the corpus must be able to sustain data loss. This is the case
if the distribution is long-tailed, but is not true if the distribution is uniform.

\begin{figure}
\includegraphics[width=\linewidth]{long-tail-ranks.png}
\caption{The frequencies of words in the Wikipedia Featured Articles corpus have a long-tailed distribution.}
\label{fig:wordDist}
\end{figure}

The distribution shown in Figure~\ref{fig:wordDist} is long-tailed. This implies that killing
tasks will impact the tail of the distribution more than the head.

\subsubsection{Precision}
Task killing should least impact the head of the word distribution.
Figure~\ref{fig:precision} shows that even with a kill rate of~0.8 and a work level of~0.4,
meaning that the cluster only looks at 8\% of the words in the corpus, the results
are at least~80\% accurate. Increasing the work level, which increases the fidelity of
results emitted by workers, also increases the fidelity of the top~100 results.


\begin{figure}
\includegraphics[width=\linewidth]{top-100-precision.png}
\caption{Fraction of the top~100 results returned in the top-100 by the word count job.}
\label{fig:precision}
\end{figure}

\subsubsection{Word Count Job Accuracy}
Because I am running a top-k job, I compute the $F_{0.5}$-score of the StreamReduce results as
opposed to the $F1$-score because I care more about precision of the top k than recall
in the tail. Figure~\ref{fig:fscore} shows that the $F_{0.5}$-score, or the weighted
accuracy of the top-100 classifier, is always higher than the fraction of the input
examined (kill rate * work level). Setting the work level to~0.4 nets a greater increase
in job accuracy versus fraction of words examined, with accuracy approximately~1.75 times
more than the fraction of words examined.

\begin{figure}
\includegraphics[width=\linewidth]{f-score.png}
\caption{$F_{0.5}$-score for the top-100 word count job. I measured the $F$-score with $\beta=0.5$ because the goal of the job is to run a top-k filter on it, meaning I care more about precision than recall.}
\label{fig:fscore}
\end{figure}

\subsection{Speedup}
StreamReduce is able to achieve a speedup over a MapReduce-style computation through three mechanisms:
providing multiple computation routines for the workers to choose from; varying the task kill
rate; and varying the target work level, which influences the chosen frequency of computation
routines per worker.

Table~\ref{table:runtime} shows the effect of varying the kill rate and work level. The row
with kill~rate~$=0.0$ and work~level~$=1.0$ represents the perfect, MapReduce-style, execution
of the word count job. The only consistent reduction in execution time is a result of
increasing the kill rate. Increasing the kill rate exhibits a linear reduction in execution
time as the fraction of processed data linearly decreases.

\begin{table}
\begin{tabularx}{\linewidth}{|c|c|X|}
\hline
Kill rate & Work level & Average execution time [minutes] \\ \hline
0.0 & 1.0 & 11:35.5 \\ \hline
0.0 & 0.4 & 10:33.8 \\ \hline
0.0 & 0.8 & 10:30.8 \\ \hline
0.2 & 0.4 & 08:25.8 \\ \hline
0.2 & 0.8 & 08:37.8 \\ \hline
0.4 & 0.4 & 06:54.0 \\ \hline
0.4 & 0.8 & 06:28.2 \\ \hline
0.6 & 0.4 & 04:14.7 \\ \hline
0.6 & 0.8 & 04:13.7 \\ \hline
0.8 & 0.4 & 02:33.3 \\ \hline
0.8 & 0.8 & 02:15.0 \\ \hline
\end{tabularx}
\caption{Impact of kill rate and work level on job execution time.}
\label{table:runtime}
\end{table}

Notably, simply by killing tasks, StreamReduce is able to achieve over a~5 times speedup compared
to the perfect execution while still returning about~84 of the top~100 words.

\section{Implementation Notes}
\label{sec:discussion}
This section describes deficiencies in the implementation of StreamReduce used to run the case study
in this paper and goals for the next version of the system.

\subsection{On-the-Fly Job Reconfiguration}
The mechanism for altering execution of a job as it is running described in Section~\ref{sec:jobfile}
is not yet implemented. Adding this feature to StreamReduce is a matter of adding an API endpoint
to the servers run by each role and adding a bin-script to submit the new Jobfile.

\subsection{Internode Communication}
In Section~\ref{sec:master} I mentioned design difficulties relating to the master and using HTTP
requests for sending messages between nodes. I initially chose to use HTTP requests with POST payloads
to send data and results between nodes because of its low barrier to entry; however, initiating many
HTTP requests in succession is expensive. When killing jobs on worker nodes, the observed time savings
was minimal because most of the time spent per datum involved shipping it from the master to the worker.

In order to mitigate the effects of this problem, I batched individual requests into groups of~5. This
lowered the number of connections I was initiating but it also limited the parallelism of the cluster
and removed workers from the pool for longer periods because they had more data to process at a time.
A killed task on the worker no longer freed it up to receive a new task immediately.
Keeping workers removed from the pool caused the queues on the master to grow longer as the workers
could no longer keep up with the rate the fetcher was producing data.

Eventually, I introduced a configuration option to kill tasks at the master. The master then
decides to discard a task as it is popped from the queue, before it is ever sent to a worker.
Doing so improved the execution time of the cluster but also meant that workers no longer had
a full idea of how accurate they were being because they never killed any tasks themselves.

In order to address this problem, the master needs to keep a persistent channel open between
it and all active nodes. Communication channels should be established when nodes come online.
Possible solutions include writing to a socket or using a higher abstraction like AMPQ\@.
AMPQ is appealing because it also relieves the master from handling queues.

\subsection{Node Health}
Currently, the master does not probe nodes to ensure that they are responsive. In the case that
a node is serving a worker role for a job, this deficiency is masked. The node will simply not
return the HTTP request sent to it to push it some data and the worker will remain removed from
the pool. If the node is in the role of fetcher or collector, the master will stall.

All messages passed by the master to other nodes must be made non-blocking.

\subsection{Job Management}
There is no support for killing jobs or removing them from the master's data structures. The only
way to remove a job is to kill the master and all other nodes in the cluster.

\section{Conclusion}
I have extended the ideas of approximate computations to a MapReduce-style framework and shown
that it is feasible to tune the input parameters of a job to achieve sufficient precision and
recall for a particular application.

I have taken a different approach to streaming MapReduce by treating data not as a batch, but
as a stream. At any given point in the execution of a job, the collectors will have accurate
(so far as the job parameters allow) results for data seen until that point. This approach
guarantees freshness and makes it easier to incorporate new data into an existing computation,
yet it suffers from being limited to online aggregations.

The StreamReduce source code is open source and available at:
\begin{center}
  \url{https://github.com/lopopolo/sr}
\end{center}

\bibliography{paper}{}
\bibliographystyle{plain}

\end{document}
